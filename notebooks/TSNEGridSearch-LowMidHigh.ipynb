{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid searching parameters for TSNE\n",
    "\n",
    "### Aim: \n",
    "Given *n* parameters, where each parameter could be of low, middle, or high value, narrow down \n",
    "the range of each parameter such that you converge on a set of parameters that gives \n",
    "**distinct clusters** when plotting the dimensionally-reduced data on a graph\n",
    "\n",
    "### General usage guide:\n",
    "\n",
    "- User will set a low value and a high value for each parameter (the middle-of-the-range value will be auto-generated)\n",
    "- Then run the rest of the notebook to:\n",
    "  - Compute the product between all low/mid/high values between each separate parameter\n",
    "  - \n",
    "\n",
    "Note: any time you change the parameters, you need to rebuild the pipelines list. If you execute from that cell and run all below in sequence, you will be doing things correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import dibs_notebook_header\n",
    "import dibs\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global runtime variables below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global runtime variables. This cell should only need to be executed once.\n",
    "num_cpu_processors = 4  # Set the maximum number of processors you are willing to use at a given time\n",
    "percent_epm_train_files_to_cluster_on = 0.3  # Set a number between 0 and 1. With larger value, train time will take longer but more data will be introduced to clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Parameter tuning section--\n",
    "\n",
    "This is where the user input goes to effect clustering outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gmm_clusters_aka_num_colours = 8  # Sets the number of clusters that GMM will try to label\n",
    "perplexity_low  = 100\n",
    "perplexity_high = 350\n",
    "early_exaggeration_low  = 10\n",
    "early_exaggeration_high = 300\n",
    "learning_rate_low  = 50\n",
    "learning_rate_high = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't touch this section\n",
    "perplexity_mid = (perplexity_low + perplexity_high)/2\n",
    "early_exaggeration_mid = (early_exaggeration_low + early_exaggeration_high) / 2\n",
    "learning_rate_mid = (learning_rate_low + learning_rate_high) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- End of parameter tuning section --\n",
    "\n",
    "---\n",
    "\n",
    "Everything below must be run from top to bottom, in sequence, to generate the \n",
    "new outputs of parameters set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generate the product between all possible parameters\n",
    "parameters_product = [\n",
    "    \n",
    "]\n",
    "kwargs_product = [\n",
    "    {'tsne_perplexity': perplexity_i,\n",
    "     'tsne_early_exaggeration': early_exaggeration_j,\n",
    "     'tsne_learning_rate': learning_rate_k,\n",
    "     \n",
    "     'gmm_n_components': num_gmm_clusters_aka_num_colours,\n",
    "     'tsne_n_components': 2, # 2D dimensionality reduction\n",
    "     'cross_validation_n_jobs': 1,\n",
    "     'classifier_n_jobs':1, \n",
    "     'tsne_n_jobs': 1,\n",
    "    } for perplexity_i, early_exaggeration_j, learning_rate_k in itertools.product(\n",
    "        [\n",
    "            perplexity_low,\n",
    "            perplexity_mid,\n",
    "            perplexity_high,\n",
    "        ],\n",
    "        [\n",
    "            early_exaggeration_low,\n",
    "            early_exaggeration_mid,\n",
    "            early_exaggeration_high,\n",
    "        ],\n",
    "        [\n",
    "            learning_rate_low,\n",
    "            learning_rate_mid,\n",
    "            learning_rate_high,\n",
    "        ])]\n",
    "pipeline_names_by_index = [f'Pipeline_{i}' for i in range(len(kwargs_product))]\n",
    "print('Number of permutations:', len(kwargs_product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queue up which data files will be added to each Pipeline\n",
    "all_files = [os.path.join(dibs.config.DEFAULT_TRAIN_DATA_DIR, file) for file in os.listdir(dibs.config.DEFAULT_TRAIN_DATA_DIR)]\n",
    "train_data = half_files = all_files[:int(len(all_files) * percent_epm_train_files_to_cluster_on)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create list of pipelines with all of the different combinations of parameters inserted\n",
    "# Do not be alarmed if you see DEBUG output as an output in the cell\n",
    "# To turn off debug output, change DIBS/config.ini [LOGGING] from DEBUG to ERROR\n",
    "pipelines_ready_for_building = [dibs.pipeline.PipelineMimic(name, **kwargs).add_train_data_source(*train_data) for name, kwargs in zip(pipeline_names_by_index, kwargs_product)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step: leveraging multiprocessing to get as much work done in as short a time as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heavy lifting/processing is done here\n",
    "start_time = time.perf_counter()\n",
    "with multiprocessing.Pool(num_cpu_processors) as pool:\n",
    "    pipelines_queued = [pool.apply_async(pipe_i.build) for pipe_i in pipelines_ready_for_building]\n",
    "    pipelines_results = [res.get() for res in pipelines_queued]\n",
    "end_time = time.perf_counter()\n",
    "print(f'Total compute time: {round((end_time-start_time)/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: evaluating \"goodness\" of a set of parameters is based on the distinctness of clusters. More distinct = better parameters set.\n",
    "for i, pipeline_i in enumerate(pipelines_results):\n",
    "    perplexity_i, learning_rate_i, early_exaggeration_i = pipeline_i.tsne_perplexity, pipeline_i.tsne_learning_rate, pipeline_i.tsne_early_exaggeration\n",
    "    print(f\"perplexity: {perplexity_i} / learning rate: {learning_rate_i} / early_exaggeration: {early_exaggeration_i} \")\n",
    "    pipeline_i.plot_clusters_by_assignments(fig_file_prefix=f'{time.strftime(\"%Y-%m-%d_%HH%MM\")}__{pipeline_i.name}__', show_now=True, save_to_file=True, figsize=(20,15),s=1.5)\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_windows",
   "language": "python",
   "name": "dibs_windows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
