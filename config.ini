
[APP]
# DEFAULT_SAVED_GRAPH_FILE_FORMAT (str): Some valid file extensions: svg, jpg . For now, you must not pick 'png'. TODO: elaborate on below var desc.
DEFAULT_SAVED_GRAPH_FILE_FORMAT = jpg
# FRAMES_OUTPUT_FORMAT (str): The extension type for which frames will be saved
FRAMES_OUTPUT_FORMAT = png
# GENERATE_VIDEOS (required, bool): if this is true, make sure direct to the video below AND that you created the two specified folders!
GENERATE_VIDEOS = True
# N_JOBS (required, int): Number of cores to use in multiprocessing steps. This value must be 1 or greater.  TODO: low: reevaluate. Currently not being used at all
N_JOBS = 2
# NUMEXPR_MAX_THREADS (optional, int): max number of threads NUMEXPR can use for math functions. Sometimes
#   this variable is not set in the os environment, so we can set it here for the project runtime if it is
#   missing. If it is indeed present in the environment, then this variable is ignored altogether.
NUMEXPR_MAX_THREADS =
# OUTPUT_VIDEO_FPS (required, int):  TODO: explain better -- originally was a variable relative to PERCENT_FRAMES_TO_LABEL
OUTPUT_VIDEO_FPS = 30
# PERCENT_FRAMES_TO_LABEL (required, float): the percent of frames in VIDEO to label and output to FRAMES output path  # TODO: evaluate how it works, and if to keep, in new model structure
PERCENT_FRAMES_TO_LABEL = 0.50
# PLOT_GRAPHS (required, bool): Change to False if you don't want plots brought up automatically.
PLOT_GRAPHS = False
# SAVE_GRAPHS_TO_FILE (required, bool):
SAVE_GRAPHS_TO_FILE = True
# VIDEO_FRAME_RATE (required, float):
VIDEO_FRAME_RATE = 30

[STREAMLIT]
# default_pipeline_location (str, optional): specify the location  # TODO: elaborate. Useful!?
default_pipeline_location =

;[VIDEO]
;stuff =

[DLC_FEATURES]
# Modify the below values to reflect the name of the body parts used in DLC which will then be parsed in package.
SNOUT/HEAD = Snout/Head
LEFT_SHOULDER/FOREPAW = Forepaw/Shoulder1
RIGHT_SHOULDER/FOREPAW = Forepaw/Shoulder2
LEFT_HIP/HINDPAW = Hindpaw/Hip1
RIGHT_HIP/HINDPAW = Hindpaw/Hip2

TAILBASE = TailBase
NOSETIP = NoseTip
FOREPAW_LEFT = ForepawLeft
FOREPAW_RIGHT = ForepawRight
HINDPAW_LEFT = HindpawLeft
HINDPAW_RIGHT = HindpawRight

[LOGGING]
### Name, format, and create log levels for the logger
### Valid log levels are limited to: CRITICAL, FATAL, ERROR, WARN, WARNING, INFO, DEBUG, NOTSET
# LOGGER_NAME (required, str):
DEFAULT_LOGGER_NAME = default_logger
# LOG_FILE_NAME (required, str): Name of the log file. The log file is always saved in .../DIBS/logs/
LOG_FILE_NAME = default.log
# LOG_FORMAT (required, str):
LOG_FORMAT = %(asctime)s - %(name)s - %(levelname)-8s - %(message)s
# STREAM_LOG_LEVEL (required, str): (Debugging: DEBUG) The log level at which log message are pushed to STDOUT
STREAM_LOG_LEVEL = INFO
# FILE_LOG_LEVEL (required):
FILE_LOG_LEVEL = DEBUG
# LOG_FILE_FOLDER_PATH (optional, absolute path): Leave LOG_FILE_FOLDER_PATH blank to use the default pathing. Otherwise,
#    fill value with an ABSOLUTE to the folder where log will be kept
LOG_FILE_FOLDER_PATH =

[MODEL]
# DEFAULT_CLASSIFIER (required, str): Valid entries include { SVM, RANDOMFOREST }
DEFAULT_CLASSIFIER = SVM
# RANDOM_STATE (required, int): Leave random_state value blank for using an actually random seed value
RANDOM_STATE = 42
# HOLDOUT_TEST_PCT (required, float):
HOLDOUT_TEST_PCT = 0.50
# CROSS_VALIDATION_K (required, int):
CROSS_VALIDATION_K = 2
# CROSS_VALIDATION_N_JOBS (required, int): Number of cores used for k-fold cross validation
#    (set to -1 to use all cores, -2 to use all cores but one)
CROSS_VALIDATION_N_JOBS = 3

[PATH]
# OUTPUT_PATH (optional, folder path): If present, must be an absolute path to an existing dir. If value is left empty, default path will be "$DIBS/output"
OUTPUT_PATH =
# DEFAULT_TRAIN_DATA_DIR (required, dir path): if value is not an absolute path,
#   value is assumed to be a directory within the DIBS project folder (same level as config.ini) and
#   is parsed as such. Absolute paths are parsed as-is.
DEFAULT_TRAIN_DATA_DIR = epm_data_csv_train
# DEFAULT_TEST_DATA_DIR (required, dir path): if value is not an absolute path,
#   value is assumed to be a directory within the DIBS project folder (same level as config.ini) and
#   is parsed as such. Absolute paths are parsed as-is.
DEFAULT_TEST_DATA_DIR = epm_data_csv_test

[TESTING]
# TEST_FILE__PipelineMimic__CSV__TRAIN_DATA_FILE (required, str): Must reside in DIBS/tests/test_data. Currently it also serves as the test file for the PipelineHowland tests too.
TEST_FILE__PipelineMimic__CSV__TRAIN_DATA_FILE = EPM-MCE-1DLC_resnet50_Maternal_EPMDec28shuffle1_180000.csv
# TEST_FILE__PipelineMimic__CSV__PREDICT_DATA_FILE (required, str): Must reside in DIBS/tests/test_data. Currently it also serves as the test file for the PipelineHowland tests too.
TEST_FILE__PipelineMimic__CSV__PREDICT_DATA_FILE = EPM-MCE-2DLC_resnet50_Maternal_EPMDec28shuffle1_180000.csv

# DEFAULT_TEST_FILE (required, file name): Must reside in: DIBS/tests/test_data  TODO: low: elaborate
DEFAULT_PIPELINE_PRIME_CSV_TEST_FILE = TruncatedSample_Video1DLC_resnet50_EPM_DLC_BSAug25shuffle1_495000.csv

# DEFAULT_H5_TEST_FILE (optional, file name): must reside in DIBS/tests/test_data  TODO: low: elaborate
DEFAULT_H5_TEST_FILE = RowsDeleted_FullSample_Video1DLC_resnet50_EPM_DLC_BSOIDAug25shuffle1_495000.h5
# max_rows_to_read_in_from_csv (optional, int): change this value to set the maximum  # TODO: deprecate since old and new way of reading max lines is different?
MAX_ROWS_TO_READ_IN_FROM_CSV = 100_000_000_000

[VIDEO]
DEFAULT_FONT_SCALE = 1
DEFAULT_TEXT_BGR = (255, 255, 255)
DEFAULT_TEXT_BACKGROUND_BGR = (0, 0, 0)


########################################################################################################################
### Classifier parameters ###
[CLASSIFIER]
# DEFAULT_CLASSIFIER (required, str): Valid entries include { SVM, RANDOMFOREST }
DEFAULT_CLASSIFIER = SVM
# VERBOSE (required, int):
VERBOSE = 0

[EM/GMM]
# n_components (required, int): n clusters (set to 30 after debugging?)  TODO: med: re-read paper and find optimal n_components
n_components = 10
# covariance_type: # Must be one of: {full, tied, diag, spherical} (see the following link for more information: https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html)
covariance_type = full
# tol (required, float): TODO: elaborate
tol = 1e-3
# reg_covar: (set to 1e-06 after debugging)
reg_covar = 1e-03
# init_params (required, str): Initialization parameters. Must be one of { random, kmeans }
init_params = kmeans
# max_iter (required, int): TODO: Elaborate
max_iter = 300
# n_init (required, int): TODO: elaborate
n_init = 20
# verbose (required, int): Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.
verbose = 0
# verbose_interval (optional, int): How often the verbose output goes to stdout
verbose_interval = 50

[HDBSCAN]
# min_samples (required, int): (authors' note: small number)
min_samples = 10

[MLP]
# activation (required, str): logistics appears to outperform tanh and relu
activation = logistic
# hidden_layer_size (required, tuple of integers): **IMPORTANT NOTE**: hidden_layer_sizes is a
#   special variable that is evaluated exactly as it is written. Thus, if it is
#   written as '(100, 10)' (without the single quotes), it will be interpreted as a tuple of integers.
hidden_layer_sizes = (100, 10)
# solver (required, str):
solver = adam
# learning_rate (required, str):
learning_rate = constant
# learning_rate_init (required, float):
learning_rate_init = 0.001
# alpha (required, float): (Original note: regularization default is better than higher values.)
alpha = 0.0001
# max_iter: (original value is 1000)
max_iter = 100
# early_stopping (required, bool):
early_stopping = False
# verbose (required, int): set verbose=1 for tuning feedforward neural network
verbose = 0

[RANDOMFOREST]
# n_estimators (required, int): TODO: elaborate
n_estimators = 50
# n_jobs (required, int): TODO: elaborate
n_jobs = 3
# verbose (rquired, int):
verbose = 0

[SVM]
# C (required, float):
C = 10
# gamma (required, float):
gamma = 0.5
# probability (required, bool): TODO: elaborate, get docs here
probability = True
# verbose (required, int): (Change back to 0 when done debugging)
verbose = 0
# n_jobs (required, int): Number of cores to use in training model.
#    n_jobs = -1 means all cores being used, set to -2 for all cores but one.
n_jobs = 3

[UMAP]
# n_neighbors (required, int): TODO: explain
n_neighbors = 100
# n_components (required, int): TODO: explain
n_components = 3
# min_dist: small value
min_dist = 0.0

[TSNE]
# https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
# early_exaggeration (required, float):
early_exaggeration = 700
# implementation (required, str): Set default implementation for TSNE usage. Valid values include: { SKLEARN, BHTSNE, OPENTSNE }   # TODO: elaborate
implementation = OPENTSNE
# init (required, str): Initialization of embedding. "PCA initialization ... is usually more
#   globally stable than random initialization.". Must one of the following: { random, pca }
init = pca
# learning_rate (optional, float): The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.
#   max(200, len(self.all_features) // 16) if not self.tsne_learning_rate else
learning_rate = 1000
# n_components (required, int):
n_components = 2
# n_iter (required, int): Set the maximum iteration that TSNE will go. A good default value is 1,000. 250 is the absolute minimum value. NOTE: if you use the "BHTSNE" implementation, "n_iter" will be ignored and it will use 1,000 regardless of settings.
n_iter = 1000
# n_jobs (required, int): The number of CPU cores that can simultaneously work on solving the TSNE clustering.
#   Note that some implementations, like BHTSNE, do not have a valid option for multiprocessing. Those
#   implementations will then be single-threaded implicitly.
n_jobs = 3
# perplexity (required, float or lambda), float): t-SNE perplexity value.
#   Test. TODO: flesh out
# perplexity = lambda self: 0.005 * self.num_training_data_points
perplexity = 800
# theta (float, required): TODO
theta = 0.5
# verbose (required, int): (original note: verbose=2 shows check points)
verbose = 0
